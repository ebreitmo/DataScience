import matplotlib.pyplot as plt
import numpy as np

# -------------------- Function warmUpExercise ------------------------                                                                         
def warmUpExercise():
 print(np.identity(5))
# print(np.eye(5))

# -------------------- Function computeCost ----------------------------
def computeCost(X, y, theta):
 m=len(y)
 Y=y.reshape(m,1)
 J=0.5/m*sum(np.square((np.dot(X,theta)-Y)))
 return (J)

# -------------------- Function gradientDescent -------------------------
def gradientDescent(X, y, theta, alpha, iterations):
 m=len(y)
 Y=y.reshape(m,1)
# For debugging print J_history
 #J_history=np.zeros((iterations,1))
 for i in range(1,iterations):
  theta = theta - alpha/m * (np.dot((np.dot(X,theta)-Y).transpose(),X)).transpose()
  #J_history[i]=computeCost(X,y,theta) 
  #print("J_history:  ",J_history[i])
 return(theta)

# ==================== Part 1: Basic Function ===========================
#  Complete warmUpExercise.m
print('Running warmUpExercise ... \n');
print('5x5 Identity Matrix: \n');
warmUpExercise()

#  ======================= Part 2: Plotting =============================
x, y = np.loadtxt('ex1data1.txt', delimiter=',', unpack=True)
m = len(y)  # Number of training examples

plt.plot(x,y, 'rx', label='Training data')
plt.xlim(xmin=0)  
plt.ylim(ymin=0)  

plt.xlabel('x')
plt.ylabel('y')
plt.title('Interesting Graph\nCheck it out')
plt.legend()
#plt.show()

# =================== Part 3: Cost and Gradient descent ==================
# Add column of ones to x
X=np.c_[np.ones(m),x]
# Size of matrix, 97x2 matrix
X.size
# Initialization of fitting parameters theta  
theta=np.zeros((2,1))   # 2x1 matrix ,  type(theta)   <type 'numpy.ndarray'>

# Settings for gradient descent
iterations=1500
alpha=0.01

print("Testing the cost function ...\n")
# Compute and display initial cost
print("With theta = [0 ; 0],  cost computed = ", computeCost(X, y, theta));
print("Expected cost value (approx) 32.07");

# Further testing of the cost function
theta=np.array([[-1],[2]])
J = computeCost(X, y, theta);
print('\With theta = [-1 ; 2], Cost computed = ', J)
print('Expected cost value (approx) 54.24\n')


print('Running Gradient Descent ...')
# Run gradient descent
theta = gradientDescent(X, y, theta, alpha, iterations)

# Print theta to screen
print('Theta found by gradient descent:')
print(theta)
print('Expected theta values (approx)')
print(' -3.6303\n  1.1664\n\n')


# Plot the linear fit
plt.plot(x,np.dot(X,theta), label='Linear regression')                                                                                                            
plt.xlim(xmin=0)                                                                                                                                          
plt.ylim(ymin=0)                                                                                                                                          
plt.xlabel('x')                                                                                                                                           
plt.ylabel('y')                                                                                                                                           
plt.title('Training data with Linear regression')
plt.legend() 
plt.show()                                                                                                                                                


# Predict values for population sizes of 35,000 and 70,000
predict1 = np.dot([1, 3.5],theta)
print('For population = 35,000, we predict a profit of ',predict1*10000)
predict2 = np.dot([1, 7],theta)
print('For population = 70,000, we predict a profit of ',predict2*10000);

